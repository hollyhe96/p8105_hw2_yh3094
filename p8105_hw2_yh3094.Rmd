---
title: "p8105_hw2_yh3094"
author: "Yun He"
date: "September 27, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

# Problem 1

## Import and clean NYC transit dataset

```{r transit_data_clean}
transit_data = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  mutate(entry = recode(entry, 'YES' = TRUE, 'NO' = FALSE)) 
transit_data
```

The dataset contains variables for line, station name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. I first imported the dataset using `read_csv`, cleaned all variable names using `janitor::clean_names`, and then preserved the variables that I wanted using `select`, and finally, I converted the entry variable from character (YES vs NO) to a logical variable (TRUE vs FALSE) using `recode`. After all the cleaning steps above, the final dataset contains `r nrow(transit_data)` rows and `r ncol(transit_data)` columns. In my opinion, these data are untidy because the route variable is spread across 11 columns. 

## Calculate the sum of distinct stations 

```{r distinct_station_sum}
nrow(distinct(transit_data, line, station_name))
```

There are 465 distinct stations. 

## Calculate the sum of ADA compliant stations

```{r ada_station_sum}
transit_data %>% 
  filter(ada == TRUE) %>% 
  distinct(line, station_name) %>% 
  nrow()
```

There are 84 ADA compliant stations. 

## Calculate the proportion of stations without vending allow entrance

```{r without_vending_prop}
entrance_without_vending = nrow(filter(transit_data, vending == "NO" & entry == TRUE))
sum_without_vending = nrow(filter(transit_data, vending == "NO"))
## calculate the proportion
entrance_without_vending/sum_without_vending
```

The proportion of station entrances / exits without vending allow entrance is 0.38.

## Reformat data 

The code chunk below reformats the dataset `transit_data` so that route number and route name are distinct variables.

```{r transit_data_reformat}
transit_data_reformat = 
  transit_data %>% 
  gather(key = route_number, value = route_name, route1:route11)
transit_data_reformat
```

## Calculate distinct stations that serve the A train

```{r a_train_sum}
transit_data_A = 
  transit_data_reformat %>% 
  filter(route_name == 'A')
nrow(distinct(transit_data_A, line, station_name))
```

There are 60 distinct stations that serve the A train.

## Calculate ADA compliant stations that serve the A train

```{r a_train_ada_sum}
filter(transit_data_A, ada == TRUE) %>% 
  distinct(line, station_name) %>% 
  nrow()
```

Of the stations that serve the A train, 17 are ADA compliant.

# Problem 2

## Import and clean the Mr. Trash Wheel sheet

```{r wheel_data_import}
wheel_data = 
  read_excel("./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx",
             sheet = "Mr. Trash Wheel", range = cell_cols("A:N")) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(sports_balls = round(sports_balls)) %>% 
  mutate(sports_balls = as.integer(sports_balls))
wheel_data
```

## Read and clean precipitation data

The code chunk below first read and clean precipitation data for 2016 and 2017 separately, then combine these two datasets, and convert month to a character variable.  

```{r precipitation_data_import}
precipiation_data_2016 = 
  read_excel("./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx", 
             sheet = "2016 Precipitation", range = cell_rows(2:14)) %>%
  janitor::clean_names() %>%
  filter(!is.na(total)) %>% 
  mutate(year = 2016) 

precipitation_data_2017 = 
  read_excel("./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx", 
             sheet = "2017 Precipitation", range = cell_rows(2:14)) %>%
  janitor::clean_names() %>%
  filter(!is.na(total)) %>% 
  mutate(year = 2017)

precipitation_data = 
  bind_rows(precipiation_data_2016, precipitation_data_2017) %>% 
  mutate(month = month.name[month]) %>% 
  select(year, everything()) ## to put year in the first column
precipitation_data
```

The Mr. Trash Wheel sheet contains `r nrow(wheel_data)` observations.And it contains variables for month, year, date, weight, volume, plastic bottles, polystyrene, cigarette_butts, glass bottles, grocery bags, chip bags, sports balls and homes powered. 

The precipitation dataset contains `r nrow(precipitation_data)` observations. And it contains variables for year, month and the total precipitaion. 

The total precipitation in 2017 is `r sum(precipitation_data_2017$total)`. The median number of sports balls in a dumpster in 2016 is `r median(filter(wheel_data, year == 2016)$sports_balls)`. 

# Problem 3

## Load the data from the `p8105.datasets` package

```{r brfss_data_import}
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
data(brfss_smart2010)
```

## Clean the BRFSS dataset

The code chunk below cleans the BRFSS dataset and creates a new variable showing the proportion of responses that were “Excellent” or “Very Good”. 

```{r brfss_data_clean}
brfss_data = 
  janitor::clean_names(brfss_smart2010) %>% 
  filter(topic == "Overall Health") %>% 
  select(-(class:question), -sample_size, -(confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  mutate(excellent_or_very_good = excellent + very_good)
```

## Answer questions related to locations

```{r location_calculation}
nrow(distinct(brfss_data, locationdesc))
nrow(distinct(brfss_data, locationabbr))
which.max(table(brfss_data$locationabbr))
```

There are 404 unique locations included in the dataset. Every state is represented. The state NJ is observed the most.

## Calculate the median “Excellent” response value in 2002

```{r excellent_median}
median(filter(brfss_data, year == 2002)$excellent, na.rm = TRUE)
```

The median of the “Excellent” response value in 2002 is 23.6.

## Make a histogram of “Excellent” response values in 2002

```{r excellent_hist}
brfss_data %>% 
  filter(year == 2002) %>% 
  ggplot(aes(x = excellent)) +
  geom_histogram() +
  labs(
    title = "The histogram of “Excellent” response value in 2002",
    x = "“Excellent” response value"
  )
```

## Make a scatterplot 

The code chunk below makes a scatterplot showing the proportion of “Excellent” response values in New York County and Queens County (both in NY State) in each year from 2002 to 2010.

```{r excellent_scatterplot}
brfss_data %>% 
  filter(locationdesc == "NY - New York County" | locationdesc == "NY - Queens County") %>%
  ggplot(aes(x = year, y = excellent, color = locationdesc)) +
  geom_point() +
  labs(
    title = "The proportion of “Excellent” response values",
    y = "The proportion of “Excellent” response value"
  )
```

